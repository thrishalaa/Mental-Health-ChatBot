{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T06:12:01.094778Z","iopub.status.busy":"2024-09-17T06:12:01.094466Z","iopub.status.idle":"2024-09-17T06:12:01.106962Z","shell.execute_reply":"2024-09-17T06:12:01.106071Z","shell.execute_reply.started":"2024-09-17T06:12:01.094743Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing dataformat1.py\n"]}],"source":["%%writefile dataformat1.py\n","\n","import json\n","\n","# Path to the input JSON file\n","input_file = '/kaggle/input/dataset1/Alexander_Street_shareGPT_2.0.json'\n","\n","# Path to the output text file\n","output_file = '/kaggle/working/data1'\n","# Load the JSON data\n","with open(input_file, 'r') as f:\n","    data = json.load(f)\n","\n","# Open the output file in write mode\n","with open(output_file, 'w') as f:\n","    for entry in data:\n","        instruction = entry['instruction']\n","        user_input = entry['input']\n","        output = entry['output']\n","        \n","        # Construct the formatted string\n","        formatted = (\n","            f\"<s>[INST] <<SYS>>\\n\"\n","            f\"{instruction}\\n\"\n","            f\"<</SYS>>\\n\"\n","            f\"{user_input} [/INST] {output} </s>\\n\"\n","        )\n","        \n","        # Write to the output file\n","        f.write(formatted)\n","\n","print(\"Conversion complete! Output saved to:\", output_file)"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-17T06:12:01.108745Z","iopub.status.busy":"2024-09-17T06:12:01.108420Z","iopub.status.idle":"2024-09-17T06:12:02.401738Z","shell.execute_reply":"2024-09-17T06:12:02.400623Z","shell.execute_reply.started":"2024-09-17T06:12:01.108712Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Conversion complete! Output saved to: /kaggle/working/data1\n"]}],"source":["!python dataformat1.py"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T06:12:05.521537Z","iopub.status.busy":"2024-09-17T06:12:05.521119Z","iopub.status.idle":"2024-09-17T06:12:05.531684Z","shell.execute_reply":"2024-09-17T06:12:05.530459Z","shell.execute_reply.started":"2024-09-17T06:12:05.521497Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing dataformat2.py\n"]}],"source":["%%writefile dataformat2.py\n","\n","import json\n","\n","# Path to the input JSON file\n","input_file = '/kaggle/input/dataset2/amod-dataset.json'\n","\n","# Path to the output text file\n","output_file = '/kaggle/working/data2'\n","\n","# Load the JSON data\n","with open(input_file, 'r') as f:\n","    data = f.readlines()\n","\n","# Open the output file in write mode\n","with open(output_file, 'w') as f:\n","    for line in data:\n","        entry = json.loads(line)\n","        context = entry['Context'].replace(\"\\u00a0\", \"\")\n","        response = entry['Response'].replace(\"\\u00a0\", \"\")\n","        instruction = \"If you are a counsellor, please answer the questions based on the description of the patient.\"\n","        \n","        # Construct the formatted string\n","        formatted = (\n","            f\"<s>[INST] <<SYS>>\\n\"\n","            f\"{instruction}\\n\"\n","            f\"<</SYS>>\\n\"\n","            f\"{context} [/INST] {response} </s>\\n\"\n","        )\n","        \n","        # Write to the output file\n","        f.write(formatted)\n","\n","print(\"Conversion complete! Output saved to:\", output_file)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T06:12:09.175787Z","iopub.status.busy":"2024-09-17T06:12:09.175186Z","iopub.status.idle":"2024-09-17T06:12:10.495387Z","shell.execute_reply":"2024-09-17T06:12:10.494420Z","shell.execute_reply.started":"2024-09-17T06:12:09.175726Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Conversion complete! Output saved to: /kaggle/working/data2\n"]}],"source":["!python dataformat2.py"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T06:12:17.910791Z","iopub.status.busy":"2024-09-17T06:12:17.909930Z","iopub.status.idle":"2024-09-17T06:12:17.916426Z","shell.execute_reply":"2024-09-17T06:12:17.915401Z","shell.execute_reply.started":"2024-09-17T06:12:17.910749Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting combineddata.py\n"]}],"source":["%%writefile combineddata.py\n","\n","# Paths to the input files\n","input_file1 = '/kaggle/working/data1'\n","input_file2 = '/kaggle/working/data2'\n","\n","# Path to the output file where both files will be combined\n","output_file = '/kaggle/working/finaldata'\n","# Open the output file in write mode\n","with open(output_file, 'w') as outfile:\n","    # Append the content of the first file\n","    with open(input_file1, 'r') as infile1:\n","        outfile.write(infile1.read())\n","    \n","    # Append the content of the second file\n","    with open(input_file2, 'r') as infile2:\n","        outfile.write(infile2.read())\n","\n","print(\"Files have been successfully combined into:\", output_file)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T06:12:21.603953Z","iopub.status.busy":"2024-09-17T06:12:21.603162Z","iopub.status.idle":"2024-09-17T06:12:22.777232Z","shell.execute_reply":"2024-09-17T06:12:22.776244Z","shell.execute_reply.started":"2024-09-17T06:12:21.603911Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files have been successfully combined into: /kaggle/working/finaldata\n"]}],"source":["!python /kaggle/working/combineddata.py"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T06:12:25.635158Z","iopub.status.busy":"2024-09-17T06:12:25.634240Z","iopub.status.idle":"2024-09-17T06:12:57.405900Z","shell.execute_reply":"2024-09-17T06:12:57.404683Z","shell.execute_reply.started":"2024-09-17T06:12:25.635115Z"},"trusted":true},"outputs":[],"source":["!pip install -q accelerate==0.34.2 peft==0.4.0 bitsandbytes==0.43.3 transformers==4.38.2 trl==0.4.7"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T14:27:36.841218Z","iopub.status.busy":"2024-09-17T14:27:36.840434Z","iopub.status.idle":"2024-09-17T14:27:51.723541Z","shell.execute_reply":"2024-09-17T14:27:51.722420Z","shell.execute_reply.started":"2024-09-17T14:27:36.841177Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T14:28:26.013734Z","iopub.status.busy":"2024-09-17T14:28:26.012489Z","iopub.status.idle":"2024-09-17T14:28:26.025383Z","shell.execute_reply":"2024-09-17T14:28:26.024276Z","shell.execute_reply.started":"2024-09-17T14:28:26.013687Z"},"trusted":true},"outputs":[],"source":["model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","# The instruction dataset to use\n","dataset_name = \"/kaggle/working/finaldata\"\n","\n","# Fine-tuned model name\n","new_model = \"Llama-finetune-therapy\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"/kaggle/working/\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T06:13:46.940548Z","iopub.status.busy":"2024-09-17T06:13:46.940145Z","iopub.status.idle":"2024-09-17T14:15:41.408758Z","shell.execute_reply":"2024-09-17T14:15:41.407821Z","shell.execute_reply.started":"2024-09-17T06:13:46.940509Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"373cde8e8755406b826840234b18aba2","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bf61e4eb4dc4bbdb728bf3b882aca81","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd4f1e1eba054e829596c1c95f74fd48","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"596ba32a7d5747cc8a8cae7aabc5ba72","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70f68531fdf0479e84970eb751814133","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"edc3d250b2f04653b173f9fdf1a6add7","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3109646690640c796442f8c04f60196","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f17f65dd861b4ccfbfc349d928deb738","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52efbcc4c42c4ec3ae3a869284847949","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6f0ee6721e34ff798fbed978ea52cc3","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afe990987c9f410aafb4b813c28a5646","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"911d0d7302424277bffdd40a6f74f5ec","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e94b5a600844268a1655bb12e5c18f6","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66dd2904677e444d89d6bd0ce8268042","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/49586 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='12397' max='12397' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12397/12397 7:59:59, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>2.835600</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>9.529900</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>2.630100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>5.257400</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>1.968600</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.310800</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>1.148300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.735700</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.223000</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.955000</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>1.298800</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.525000</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>1.108000</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.164000</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>1.075100</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.456300</td>\n","    </tr>\n","    <tr>\n","      <td>425</td>\n","      <td>0.961300</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.863100</td>\n","    </tr>\n","    <tr>\n","      <td>475</td>\n","      <td>1.185600</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.856200</td>\n","    </tr>\n","    <tr>\n","      <td>525</td>\n","      <td>1.242700</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.714200</td>\n","    </tr>\n","    <tr>\n","      <td>575</td>\n","      <td>1.055400</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.787800</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>0.999300</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.975800</td>\n","    </tr>\n","    <tr>\n","      <td>675</td>\n","      <td>1.308100</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.641700</td>\n","    </tr>\n","    <tr>\n","      <td>725</td>\n","      <td>1.461700</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.973600</td>\n","    </tr>\n","    <tr>\n","      <td>775</td>\n","      <td>1.191500</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.834600</td>\n","    </tr>\n","    <tr>\n","      <td>825</td>\n","      <td>1.049700</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.985700</td>\n","    </tr>\n","    <tr>\n","      <td>875</td>\n","      <td>1.210400</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.745800</td>\n","    </tr>\n","    <tr>\n","      <td>925</td>\n","      <td>1.077500</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.486100</td>\n","    </tr>\n","    <tr>\n","      <td>975</td>\n","      <td>1.193900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.846800</td>\n","    </tr>\n","    <tr>\n","      <td>1025</td>\n","      <td>1.009900</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.806800</td>\n","    </tr>\n","    <tr>\n","      <td>1075</td>\n","      <td>1.061800</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.806000</td>\n","    </tr>\n","    <tr>\n","      <td>1125</td>\n","      <td>0.982400</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>0.820500</td>\n","    </tr>\n","    <tr>\n","      <td>1175</td>\n","      <td>1.053100</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.613800</td>\n","    </tr>\n","    <tr>\n","      <td>1225</td>\n","      <td>1.152300</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.761000</td>\n","    </tr>\n","    <tr>\n","      <td>1275</td>\n","      <td>0.818400</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.836500</td>\n","    </tr>\n","    <tr>\n","      <td>1325</td>\n","      <td>1.159600</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>0.611200</td>\n","    </tr>\n","    <tr>\n","      <td>1375</td>\n","      <td>1.026400</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.899900</td>\n","    </tr>\n","    <tr>\n","      <td>1425</td>\n","      <td>1.176400</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>0.594000</td>\n","    </tr>\n","    <tr>\n","      <td>1475</td>\n","      <td>1.195000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.524000</td>\n","    </tr>\n","    <tr>\n","      <td>1525</td>\n","      <td>1.131200</td>\n","    </tr>\n","    <tr>\n","      <td>1550</td>\n","      <td>0.513900</td>\n","    </tr>\n","    <tr>\n","      <td>1575</td>\n","      <td>1.092800</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.509000</td>\n","    </tr>\n","    <tr>\n","      <td>1625</td>\n","      <td>1.194700</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>0.696100</td>\n","    </tr>\n","    <tr>\n","      <td>1675</td>\n","      <td>1.251200</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.310300</td>\n","    </tr>\n","    <tr>\n","      <td>1725</td>\n","      <td>1.080300</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.462500</td>\n","    </tr>\n","    <tr>\n","      <td>1775</td>\n","      <td>0.916200</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.707300</td>\n","    </tr>\n","    <tr>\n","      <td>1825</td>\n","      <td>1.167600</td>\n","    </tr>\n","    <tr>\n","      <td>1850</td>\n","      <td>0.572300</td>\n","    </tr>\n","    <tr>\n","      <td>1875</td>\n","      <td>0.989400</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.628200</td>\n","    </tr>\n","    <tr>\n","      <td>1925</td>\n","      <td>1.091000</td>\n","    </tr>\n","    <tr>\n","      <td>1950</td>\n","      <td>0.420800</td>\n","    </tr>\n","    <tr>\n","      <td>1975</td>\n","      <td>0.872100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.680300</td>\n","    </tr>\n","    <tr>\n","      <td>2025</td>\n","      <td>1.213800</td>\n","    </tr>\n","    <tr>\n","      <td>2050</td>\n","      <td>0.647700</td>\n","    </tr>\n","    <tr>\n","      <td>2075</td>\n","      <td>1.059000</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.618000</td>\n","    </tr>\n","    <tr>\n","      <td>2125</td>\n","      <td>1.137700</td>\n","    </tr>\n","    <tr>\n","      <td>2150</td>\n","      <td>0.340700</td>\n","    </tr>\n","    <tr>\n","      <td>2175</td>\n","      <td>1.286800</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.428900</td>\n","    </tr>\n","    <tr>\n","      <td>2225</td>\n","      <td>1.308500</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.347600</td>\n","    </tr>\n","    <tr>\n","      <td>2275</td>\n","      <td>1.012300</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.586500</td>\n","    </tr>\n","    <tr>\n","      <td>2325</td>\n","      <td>1.170200</td>\n","    </tr>\n","    <tr>\n","      <td>2350</td>\n","      <td>0.435200</td>\n","    </tr>\n","    <tr>\n","      <td>2375</td>\n","      <td>1.311200</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.644500</td>\n","    </tr>\n","    <tr>\n","      <td>2425</td>\n","      <td>1.246400</td>\n","    </tr>\n","    <tr>\n","      <td>2450</td>\n","      <td>0.519100</td>\n","    </tr>\n","    <tr>\n","      <td>2475</td>\n","      <td>1.079700</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.495400</td>\n","    </tr>\n","    <tr>\n","      <td>2525</td>\n","      <td>1.163400</td>\n","    </tr>\n","    <tr>\n","      <td>2550</td>\n","      <td>0.543600</td>\n","    </tr>\n","    <tr>\n","      <td>2575</td>\n","      <td>1.032400</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.486000</td>\n","    </tr>\n","    <tr>\n","      <td>2625</td>\n","      <td>0.977400</td>\n","    </tr>\n","    <tr>\n","      <td>2650</td>\n","      <td>0.571700</td>\n","    </tr>\n","    <tr>\n","      <td>2675</td>\n","      <td>1.208500</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.464300</td>\n","    </tr>\n","    <tr>\n","      <td>2725</td>\n","      <td>1.049400</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.557400</td>\n","    </tr>\n","    <tr>\n","      <td>2775</td>\n","      <td>1.079300</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.680000</td>\n","    </tr>\n","    <tr>\n","      <td>2825</td>\n","      <td>1.013800</td>\n","    </tr>\n","    <tr>\n","      <td>2850</td>\n","      <td>0.434200</td>\n","    </tr>\n","    <tr>\n","      <td>2875</td>\n","      <td>1.089000</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.393000</td>\n","    </tr>\n","    <tr>\n","      <td>2925</td>\n","      <td>0.987600</td>\n","    </tr>\n","    <tr>\n","      <td>2950</td>\n","      <td>0.558600</td>\n","    </tr>\n","    <tr>\n","      <td>2975</td>\n","      <td>1.352300</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.660700</td>\n","    </tr>\n","    <tr>\n","      <td>3025</td>\n","      <td>1.124700</td>\n","    </tr>\n","    <tr>\n","      <td>3050</td>\n","      <td>0.446700</td>\n","    </tr>\n","    <tr>\n","      <td>3075</td>\n","      <td>0.994500</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.642800</td>\n","    </tr>\n","    <tr>\n","      <td>3125</td>\n","      <td>0.990100</td>\n","    </tr>\n","    <tr>\n","      <td>3150</td>\n","      <td>0.421800</td>\n","    </tr>\n","    <tr>\n","      <td>3175</td>\n","      <td>0.737400</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.435900</td>\n","    </tr>\n","    <tr>\n","      <td>3225</td>\n","      <td>1.096100</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.447000</td>\n","    </tr>\n","    <tr>\n","      <td>3275</td>\n","      <td>0.972000</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.562400</td>\n","    </tr>\n","    <tr>\n","      <td>3325</td>\n","      <td>1.023400</td>\n","    </tr>\n","    <tr>\n","      <td>3350</td>\n","      <td>0.388800</td>\n","    </tr>\n","    <tr>\n","      <td>3375</td>\n","      <td>1.131000</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.477800</td>\n","    </tr>\n","    <tr>\n","      <td>3425</td>\n","      <td>0.983400</td>\n","    </tr>\n","    <tr>\n","      <td>3450</td>\n","      <td>0.407700</td>\n","    </tr>\n","    <tr>\n","      <td>3475</td>\n","      <td>0.934000</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.523100</td>\n","    </tr>\n","    <tr>\n","      <td>3525</td>\n","      <td>0.931200</td>\n","    </tr>\n","    <tr>\n","      <td>3550</td>\n","      <td>0.555000</td>\n","    </tr>\n","    <tr>\n","      <td>3575</td>\n","      <td>1.026000</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.448300</td>\n","    </tr>\n","    <tr>\n","      <td>3625</td>\n","      <td>1.061500</td>\n","    </tr>\n","    <tr>\n","      <td>3650</td>\n","      <td>0.473000</td>\n","    </tr>\n","    <tr>\n","      <td>3675</td>\n","      <td>1.175100</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.721400</td>\n","    </tr>\n","    <tr>\n","      <td>3725</td>\n","      <td>1.050200</td>\n","    </tr>\n","    <tr>\n","      <td>3750</td>\n","      <td>0.391200</td>\n","    </tr>\n","    <tr>\n","      <td>3775</td>\n","      <td>0.969600</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.556700</td>\n","    </tr>\n","    <tr>\n","      <td>3825</td>\n","      <td>1.189700</td>\n","    </tr>\n","    <tr>\n","      <td>3850</td>\n","      <td>0.550000</td>\n","    </tr>\n","    <tr>\n","      <td>3875</td>\n","      <td>0.907800</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>0.337000</td>\n","    </tr>\n","    <tr>\n","      <td>3925</td>\n","      <td>0.956600</td>\n","    </tr>\n","    <tr>\n","      <td>3950</td>\n","      <td>0.375900</td>\n","    </tr>\n","    <tr>\n","      <td>3975</td>\n","      <td>1.206900</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.443000</td>\n","    </tr>\n","    <tr>\n","      <td>4025</td>\n","      <td>1.129000</td>\n","    </tr>\n","    <tr>\n","      <td>4050</td>\n","      <td>0.423400</td>\n","    </tr>\n","    <tr>\n","      <td>4075</td>\n","      <td>0.982300</td>\n","    </tr>\n","    <tr>\n","      <td>4100</td>\n","      <td>0.417000</td>\n","    </tr>\n","    <tr>\n","      <td>4125</td>\n","      <td>0.987400</td>\n","    </tr>\n","    <tr>\n","      <td>4150</td>\n","      <td>0.545200</td>\n","    </tr>\n","    <tr>\n","      <td>4175</td>\n","      <td>1.161000</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.506400</td>\n","    </tr>\n","    <tr>\n","      <td>4225</td>\n","      <td>1.208300</td>\n","    </tr>\n","    <tr>\n","      <td>4250</td>\n","      <td>0.343600</td>\n","    </tr>\n","    <tr>\n","      <td>4275</td>\n","      <td>1.211700</td>\n","    </tr>\n","    <tr>\n","      <td>4300</td>\n","      <td>0.316200</td>\n","    </tr>\n","    <tr>\n","      <td>4325</td>\n","      <td>1.089700</td>\n","    </tr>\n","    <tr>\n","      <td>4350</td>\n","      <td>0.372900</td>\n","    </tr>\n","    <tr>\n","      <td>4375</td>\n","      <td>1.299000</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>0.406000</td>\n","    </tr>\n","    <tr>\n","      <td>4425</td>\n","      <td>1.053300</td>\n","    </tr>\n","    <tr>\n","      <td>4450</td>\n","      <td>0.334200</td>\n","    </tr>\n","    <tr>\n","      <td>4475</td>\n","      <td>1.028600</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.388900</td>\n","    </tr>\n","    <tr>\n","      <td>4525</td>\n","      <td>1.039700</td>\n","    </tr>\n","    <tr>\n","      <td>4550</td>\n","      <td>0.408000</td>\n","    </tr>\n","    <tr>\n","      <td>4575</td>\n","      <td>0.895500</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>0.327800</td>\n","    </tr>\n","    <tr>\n","      <td>4625</td>\n","      <td>1.067500</td>\n","    </tr>\n","    <tr>\n","      <td>4650</td>\n","      <td>0.327700</td>\n","    </tr>\n","    <tr>\n","      <td>4675</td>\n","      <td>0.993500</td>\n","    </tr>\n","    <tr>\n","      <td>4700</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>4725</td>\n","      <td>1.155900</td>\n","    </tr>\n","    <tr>\n","      <td>4750</td>\n","      <td>0.595800</td>\n","    </tr>\n","    <tr>\n","      <td>4775</td>\n","      <td>0.927100</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.444600</td>\n","    </tr>\n","    <tr>\n","      <td>4825</td>\n","      <td>1.255000</td>\n","    </tr>\n","    <tr>\n","      <td>4850</td>\n","      <td>0.307000</td>\n","    </tr>\n","    <tr>\n","      <td>4875</td>\n","      <td>1.215100</td>\n","    </tr>\n","    <tr>\n","      <td>4900</td>\n","      <td>0.372400</td>\n","    </tr>\n","    <tr>\n","      <td>4925</td>\n","      <td>1.075100</td>\n","    </tr>\n","    <tr>\n","      <td>4950</td>\n","      <td>0.438400</td>\n","    </tr>\n","    <tr>\n","      <td>4975</td>\n","      <td>0.878100</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.440600</td>\n","    </tr>\n","    <tr>\n","      <td>5025</td>\n","      <td>0.973200</td>\n","    </tr>\n","    <tr>\n","      <td>5050</td>\n","      <td>0.578700</td>\n","    </tr>\n","    <tr>\n","      <td>5075</td>\n","      <td>0.923700</td>\n","    </tr>\n","    <tr>\n","      <td>5100</td>\n","      <td>0.553700</td>\n","    </tr>\n","    <tr>\n","      <td>5125</td>\n","      <td>1.015600</td>\n","    </tr>\n","    <tr>\n","      <td>5150</td>\n","      <td>0.732200</td>\n","    </tr>\n","    <tr>\n","      <td>5175</td>\n","      <td>0.980100</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>0.475600</td>\n","    </tr>\n","    <tr>\n","      <td>5225</td>\n","      <td>1.033700</td>\n","    </tr>\n","    <tr>\n","      <td>5250</td>\n","      <td>0.470900</td>\n","    </tr>\n","    <tr>\n","      <td>5275</td>\n","      <td>0.966500</td>\n","    </tr>\n","    <tr>\n","      <td>5300</td>\n","      <td>0.530100</td>\n","    </tr>\n","    <tr>\n","      <td>5325</td>\n","      <td>0.955300</td>\n","    </tr>\n","    <tr>\n","      <td>5350</td>\n","      <td>0.363600</td>\n","    </tr>\n","    <tr>\n","      <td>5375</td>\n","      <td>1.003600</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>0.376500</td>\n","    </tr>\n","    <tr>\n","      <td>5425</td>\n","      <td>0.974300</td>\n","    </tr>\n","    <tr>\n","      <td>5450</td>\n","      <td>0.364300</td>\n","    </tr>\n","    <tr>\n","      <td>5475</td>\n","      <td>0.959100</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.472700</td>\n","    </tr>\n","    <tr>\n","      <td>5525</td>\n","      <td>0.917100</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>0.593800</td>\n","    </tr>\n","    <tr>\n","      <td>5575</td>\n","      <td>1.129700</td>\n","    </tr>\n","    <tr>\n","      <td>5600</td>\n","      <td>0.424300</td>\n","    </tr>\n","    <tr>\n","      <td>5625</td>\n","      <td>0.906300</td>\n","    </tr>\n","    <tr>\n","      <td>5650</td>\n","      <td>0.431800</td>\n","    </tr>\n","    <tr>\n","      <td>5675</td>\n","      <td>1.016700</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.303300</td>\n","    </tr>\n","    <tr>\n","      <td>5725</td>\n","      <td>1.251700</td>\n","    </tr>\n","    <tr>\n","      <td>5750</td>\n","      <td>0.337900</td>\n","    </tr>\n","    <tr>\n","      <td>5775</td>\n","      <td>0.991000</td>\n","    </tr>\n","    <tr>\n","      <td>5800</td>\n","      <td>0.292600</td>\n","    </tr>\n","    <tr>\n","      <td>5825</td>\n","      <td>1.217600</td>\n","    </tr>\n","    <tr>\n","      <td>5850</td>\n","      <td>0.376100</td>\n","    </tr>\n","    <tr>\n","      <td>5875</td>\n","      <td>1.109900</td>\n","    </tr>\n","    <tr>\n","      <td>5900</td>\n","      <td>0.310900</td>\n","    </tr>\n","    <tr>\n","      <td>5925</td>\n","      <td>1.131300</td>\n","    </tr>\n","    <tr>\n","      <td>5950</td>\n","      <td>0.679400</td>\n","    </tr>\n","    <tr>\n","      <td>5975</td>\n","      <td>1.078700</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.308400</td>\n","    </tr>\n","    <tr>\n","      <td>6025</td>\n","      <td>1.191200</td>\n","    </tr>\n","    <tr>\n","      <td>6050</td>\n","      <td>0.301900</td>\n","    </tr>\n","    <tr>\n","      <td>6075</td>\n","      <td>1.081700</td>\n","    </tr>\n","    <tr>\n","      <td>6100</td>\n","      <td>0.536100</td>\n","    </tr>\n","    <tr>\n","      <td>6125</td>\n","      <td>1.057600</td>\n","    </tr>\n","    <tr>\n","      <td>6150</td>\n","      <td>0.346300</td>\n","    </tr>\n","    <tr>\n","      <td>6175</td>\n","      <td>0.907200</td>\n","    </tr>\n","    <tr>\n","      <td>6200</td>\n","      <td>0.479700</td>\n","    </tr>\n","    <tr>\n","      <td>6225</td>\n","      <td>1.036500</td>\n","    </tr>\n","    <tr>\n","      <td>6250</td>\n","      <td>0.374100</td>\n","    </tr>\n","    <tr>\n","      <td>6275</td>\n","      <td>1.037700</td>\n","    </tr>\n","    <tr>\n","      <td>6300</td>\n","      <td>0.315800</td>\n","    </tr>\n","    <tr>\n","      <td>6325</td>\n","      <td>1.259100</td>\n","    </tr>\n","    <tr>\n","      <td>6350</td>\n","      <td>0.345900</td>\n","    </tr>\n","    <tr>\n","      <td>6375</td>\n","      <td>0.927200</td>\n","    </tr>\n","    <tr>\n","      <td>6400</td>\n","      <td>0.429300</td>\n","    </tr>\n","    <tr>\n","      <td>6425</td>\n","      <td>0.996900</td>\n","    </tr>\n","    <tr>\n","      <td>6450</td>\n","      <td>0.622000</td>\n","    </tr>\n","    <tr>\n","      <td>6475</td>\n","      <td>0.947100</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.468800</td>\n","    </tr>\n","    <tr>\n","      <td>6525</td>\n","      <td>0.946000</td>\n","    </tr>\n","    <tr>\n","      <td>6550</td>\n","      <td>0.297700</td>\n","    </tr>\n","    <tr>\n","      <td>6575</td>\n","      <td>1.001700</td>\n","    </tr>\n","    <tr>\n","      <td>6600</td>\n","      <td>0.494400</td>\n","    </tr>\n","    <tr>\n","      <td>6625</td>\n","      <td>1.126200</td>\n","    </tr>\n","    <tr>\n","      <td>6650</td>\n","      <td>0.403500</td>\n","    </tr>\n","    <tr>\n","      <td>6675</td>\n","      <td>1.178700</td>\n","    </tr>\n","    <tr>\n","      <td>6700</td>\n","      <td>0.321800</td>\n","    </tr>\n","    <tr>\n","      <td>6725</td>\n","      <td>1.241000</td>\n","    </tr>\n","    <tr>\n","      <td>6750</td>\n","      <td>0.384300</td>\n","    </tr>\n","    <tr>\n","      <td>6775</td>\n","      <td>1.009000</td>\n","    </tr>\n","    <tr>\n","      <td>6800</td>\n","      <td>0.296800</td>\n","    </tr>\n","    <tr>\n","      <td>6825</td>\n","      <td>0.853300</td>\n","    </tr>\n","    <tr>\n","      <td>6850</td>\n","      <td>0.582200</td>\n","    </tr>\n","    <tr>\n","      <td>6875</td>\n","      <td>0.975900</td>\n","    </tr>\n","    <tr>\n","      <td>6900</td>\n","      <td>0.390700</td>\n","    </tr>\n","    <tr>\n","      <td>6925</td>\n","      <td>1.267300</td>\n","    </tr>\n","    <tr>\n","      <td>6950</td>\n","      <td>0.314900</td>\n","    </tr>\n","    <tr>\n","      <td>6975</td>\n","      <td>0.908000</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.451600</td>\n","    </tr>\n","    <tr>\n","      <td>7025</td>\n","      <td>0.974900</td>\n","    </tr>\n","    <tr>\n","      <td>7050</td>\n","      <td>0.372100</td>\n","    </tr>\n","    <tr>\n","      <td>7075</td>\n","      <td>1.105600</td>\n","    </tr>\n","    <tr>\n","      <td>7100</td>\n","      <td>0.365300</td>\n","    </tr>\n","    <tr>\n","      <td>7125</td>\n","      <td>0.955900</td>\n","    </tr>\n","    <tr>\n","      <td>7150</td>\n","      <td>0.533000</td>\n","    </tr>\n","    <tr>\n","      <td>7175</td>\n","      <td>0.904700</td>\n","    </tr>\n","    <tr>\n","      <td>7200</td>\n","      <td>0.566800</td>\n","    </tr>\n","    <tr>\n","      <td>7225</td>\n","      <td>1.076800</td>\n","    </tr>\n","    <tr>\n","      <td>7250</td>\n","      <td>0.283000</td>\n","    </tr>\n","    <tr>\n","      <td>7275</td>\n","      <td>1.161000</td>\n","    </tr>\n","    <tr>\n","      <td>7300</td>\n","      <td>0.291200</td>\n","    </tr>\n","    <tr>\n","      <td>7325</td>\n","      <td>1.196100</td>\n","    </tr>\n","    <tr>\n","      <td>7350</td>\n","      <td>0.331100</td>\n","    </tr>\n","    <tr>\n","      <td>7375</td>\n","      <td>1.175500</td>\n","    </tr>\n","    <tr>\n","      <td>7400</td>\n","      <td>0.322400</td>\n","    </tr>\n","    <tr>\n","      <td>7425</td>\n","      <td>0.886400</td>\n","    </tr>\n","    <tr>\n","      <td>7450</td>\n","      <td>0.306000</td>\n","    </tr>\n","    <tr>\n","      <td>7475</td>\n","      <td>1.094400</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.451200</td>\n","    </tr>\n","    <tr>\n","      <td>7525</td>\n","      <td>0.997100</td>\n","    </tr>\n","    <tr>\n","      <td>7550</td>\n","      <td>0.284900</td>\n","    </tr>\n","    <tr>\n","      <td>7575</td>\n","      <td>1.123200</td>\n","    </tr>\n","    <tr>\n","      <td>7600</td>\n","      <td>0.432900</td>\n","    </tr>\n","    <tr>\n","      <td>7625</td>\n","      <td>1.104700</td>\n","    </tr>\n","    <tr>\n","      <td>7650</td>\n","      <td>0.286800</td>\n","    </tr>\n","    <tr>\n","      <td>7675</td>\n","      <td>1.075600</td>\n","    </tr>\n","    <tr>\n","      <td>7700</td>\n","      <td>0.293700</td>\n","    </tr>\n","    <tr>\n","      <td>7725</td>\n","      <td>0.980500</td>\n","    </tr>\n","    <tr>\n","      <td>7750</td>\n","      <td>0.513100</td>\n","    </tr>\n","    <tr>\n","      <td>7775</td>\n","      <td>0.869300</td>\n","    </tr>\n","    <tr>\n","      <td>7800</td>\n","      <td>0.390000</td>\n","    </tr>\n","    <tr>\n","      <td>7825</td>\n","      <td>1.102100</td>\n","    </tr>\n","    <tr>\n","      <td>7850</td>\n","      <td>0.285000</td>\n","    </tr>\n","    <tr>\n","      <td>7875</td>\n","      <td>0.890300</td>\n","    </tr>\n","    <tr>\n","      <td>7900</td>\n","      <td>0.424100</td>\n","    </tr>\n","    <tr>\n","      <td>7925</td>\n","      <td>0.925900</td>\n","    </tr>\n","    <tr>\n","      <td>7950</td>\n","      <td>0.298900</td>\n","    </tr>\n","    <tr>\n","      <td>7975</td>\n","      <td>0.981700</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.395600</td>\n","    </tr>\n","    <tr>\n","      <td>8025</td>\n","      <td>1.234200</td>\n","    </tr>\n","    <tr>\n","      <td>8050</td>\n","      <td>0.291300</td>\n","    </tr>\n","    <tr>\n","      <td>8075</td>\n","      <td>0.886200</td>\n","    </tr>\n","    <tr>\n","      <td>8100</td>\n","      <td>0.475400</td>\n","    </tr>\n","    <tr>\n","      <td>8125</td>\n","      <td>0.916800</td>\n","    </tr>\n","    <tr>\n","      <td>8150</td>\n","      <td>0.477800</td>\n","    </tr>\n","    <tr>\n","      <td>8175</td>\n","      <td>0.918900</td>\n","    </tr>\n","    <tr>\n","      <td>8200</td>\n","      <td>0.380000</td>\n","    </tr>\n","    <tr>\n","      <td>8225</td>\n","      <td>1.085000</td>\n","    </tr>\n","    <tr>\n","      <td>8250</td>\n","      <td>0.611100</td>\n","    </tr>\n","    <tr>\n","      <td>8275</td>\n","      <td>0.934500</td>\n","    </tr>\n","    <tr>\n","      <td>8300</td>\n","      <td>0.673000</td>\n","    </tr>\n","    <tr>\n","      <td>8325</td>\n","      <td>0.812000</td>\n","    </tr>\n","    <tr>\n","      <td>8350</td>\n","      <td>0.489700</td>\n","    </tr>\n","    <tr>\n","      <td>8375</td>\n","      <td>0.971400</td>\n","    </tr>\n","    <tr>\n","      <td>8400</td>\n","      <td>0.486900</td>\n","    </tr>\n","    <tr>\n","      <td>8425</td>\n","      <td>0.903300</td>\n","    </tr>\n","    <tr>\n","      <td>8450</td>\n","      <td>0.351700</td>\n","    </tr>\n","    <tr>\n","      <td>8475</td>\n","      <td>1.067500</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.423100</td>\n","    </tr>\n","    <tr>\n","      <td>8525</td>\n","      <td>0.915800</td>\n","    </tr>\n","    <tr>\n","      <td>8550</td>\n","      <td>0.419300</td>\n","    </tr>\n","    <tr>\n","      <td>8575</td>\n","      <td>0.869500</td>\n","    </tr>\n","    <tr>\n","      <td>8600</td>\n","      <td>0.557800</td>\n","    </tr>\n","    <tr>\n","      <td>8625</td>\n","      <td>1.104700</td>\n","    </tr>\n","    <tr>\n","      <td>8650</td>\n","      <td>0.270600</td>\n","    </tr>\n","    <tr>\n","      <td>8675</td>\n","      <td>1.005900</td>\n","    </tr>\n","    <tr>\n","      <td>8700</td>\n","      <td>0.448300</td>\n","    </tr>\n","    <tr>\n","      <td>8725</td>\n","      <td>0.909900</td>\n","    </tr>\n","    <tr>\n","      <td>8750</td>\n","      <td>0.424300</td>\n","    </tr>\n","    <tr>\n","      <td>8775</td>\n","      <td>1.116200</td>\n","    </tr>\n","    <tr>\n","      <td>8800</td>\n","      <td>0.407900</td>\n","    </tr>\n","    <tr>\n","      <td>8825</td>\n","      <td>1.074100</td>\n","    </tr>\n","    <tr>\n","      <td>8850</td>\n","      <td>0.420900</td>\n","    </tr>\n","    <tr>\n","      <td>8875</td>\n","      <td>0.926000</td>\n","    </tr>\n","    <tr>\n","      <td>8900</td>\n","      <td>0.287100</td>\n","    </tr>\n","    <tr>\n","      <td>8925</td>\n","      <td>1.018700</td>\n","    </tr>\n","    <tr>\n","      <td>8950</td>\n","      <td>0.268400</td>\n","    </tr>\n","    <tr>\n","      <td>8975</td>\n","      <td>0.963100</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.384200</td>\n","    </tr>\n","    <tr>\n","      <td>9025</td>\n","      <td>0.925900</td>\n","    </tr>\n","    <tr>\n","      <td>9050</td>\n","      <td>0.611500</td>\n","    </tr>\n","    <tr>\n","      <td>9075</td>\n","      <td>1.114200</td>\n","    </tr>\n","    <tr>\n","      <td>9100</td>\n","      <td>0.308200</td>\n","    </tr>\n","    <tr>\n","      <td>9125</td>\n","      <td>0.933300</td>\n","    </tr>\n","    <tr>\n","      <td>9150</td>\n","      <td>0.285800</td>\n","    </tr>\n","    <tr>\n","      <td>9175</td>\n","      <td>0.933300</td>\n","    </tr>\n","    <tr>\n","      <td>9200</td>\n","      <td>0.697100</td>\n","    </tr>\n","    <tr>\n","      <td>9225</td>\n","      <td>0.969500</td>\n","    </tr>\n","    <tr>\n","      <td>9250</td>\n","      <td>0.490400</td>\n","    </tr>\n","    <tr>\n","      <td>9275</td>\n","      <td>1.119900</td>\n","    </tr>\n","    <tr>\n","      <td>9300</td>\n","      <td>0.374500</td>\n","    </tr>\n","    <tr>\n","      <td>9325</td>\n","      <td>0.967900</td>\n","    </tr>\n","    <tr>\n","      <td>9350</td>\n","      <td>0.420200</td>\n","    </tr>\n","    <tr>\n","      <td>9375</td>\n","      <td>1.015100</td>\n","    </tr>\n","    <tr>\n","      <td>9400</td>\n","      <td>0.281200</td>\n","    </tr>\n","    <tr>\n","      <td>9425</td>\n","      <td>1.004500</td>\n","    </tr>\n","    <tr>\n","      <td>9450</td>\n","      <td>0.436800</td>\n","    </tr>\n","    <tr>\n","      <td>9475</td>\n","      <td>1.066500</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.264700</td>\n","    </tr>\n","    <tr>\n","      <td>9525</td>\n","      <td>1.132600</td>\n","    </tr>\n","    <tr>\n","      <td>9550</td>\n","      <td>0.318000</td>\n","    </tr>\n","    <tr>\n","      <td>9575</td>\n","      <td>0.979500</td>\n","    </tr>\n","    <tr>\n","      <td>9600</td>\n","      <td>0.269500</td>\n","    </tr>\n","    <tr>\n","      <td>9625</td>\n","      <td>0.982700</td>\n","    </tr>\n","    <tr>\n","      <td>9650</td>\n","      <td>0.368000</td>\n","    </tr>\n","    <tr>\n","      <td>9675</td>\n","      <td>0.742100</td>\n","    </tr>\n","    <tr>\n","      <td>9700</td>\n","      <td>0.296500</td>\n","    </tr>\n","    <tr>\n","      <td>9725</td>\n","      <td>1.116000</td>\n","    </tr>\n","    <tr>\n","      <td>9750</td>\n","      <td>0.381200</td>\n","    </tr>\n","    <tr>\n","      <td>9775</td>\n","      <td>1.042600</td>\n","    </tr>\n","    <tr>\n","      <td>9800</td>\n","      <td>0.739100</td>\n","    </tr>\n","    <tr>\n","      <td>9825</td>\n","      <td>1.075300</td>\n","    </tr>\n","    <tr>\n","      <td>9850</td>\n","      <td>0.272400</td>\n","    </tr>\n","    <tr>\n","      <td>9875</td>\n","      <td>0.920000</td>\n","    </tr>\n","    <tr>\n","      <td>9900</td>\n","      <td>0.468400</td>\n","    </tr>\n","    <tr>\n","      <td>9925</td>\n","      <td>1.228700</td>\n","    </tr>\n","    <tr>\n","      <td>9950</td>\n","      <td>0.276200</td>\n","    </tr>\n","    <tr>\n","      <td>9975</td>\n","      <td>0.969500</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.334300</td>\n","    </tr>\n","    <tr>\n","      <td>10025</td>\n","      <td>0.974400</td>\n","    </tr>\n","    <tr>\n","      <td>10050</td>\n","      <td>0.321500</td>\n","    </tr>\n","    <tr>\n","      <td>10075</td>\n","      <td>0.969400</td>\n","    </tr>\n","    <tr>\n","      <td>10100</td>\n","      <td>0.465200</td>\n","    </tr>\n","    <tr>\n","      <td>10125</td>\n","      <td>1.082200</td>\n","    </tr>\n","    <tr>\n","      <td>10150</td>\n","      <td>0.281400</td>\n","    </tr>\n","    <tr>\n","      <td>10175</td>\n","      <td>1.045600</td>\n","    </tr>\n","    <tr>\n","      <td>10200</td>\n","      <td>0.358700</td>\n","    </tr>\n","    <tr>\n","      <td>10225</td>\n","      <td>0.853500</td>\n","    </tr>\n","    <tr>\n","      <td>10250</td>\n","      <td>0.420600</td>\n","    </tr>\n","    <tr>\n","      <td>10275</td>\n","      <td>0.980400</td>\n","    </tr>\n","    <tr>\n","      <td>10300</td>\n","      <td>0.502700</td>\n","    </tr>\n","    <tr>\n","      <td>10325</td>\n","      <td>1.122500</td>\n","    </tr>\n","    <tr>\n","      <td>10350</td>\n","      <td>0.342700</td>\n","    </tr>\n","    <tr>\n","      <td>10375</td>\n","      <td>0.882700</td>\n","    </tr>\n","    <tr>\n","      <td>10400</td>\n","      <td>0.263300</td>\n","    </tr>\n","    <tr>\n","      <td>10425</td>\n","      <td>1.063700</td>\n","    </tr>\n","    <tr>\n","      <td>10450</td>\n","      <td>0.310400</td>\n","    </tr>\n","    <tr>\n","      <td>10475</td>\n","      <td>1.187300</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.411500</td>\n","    </tr>\n","    <tr>\n","      <td>10525</td>\n","      <td>0.987700</td>\n","    </tr>\n","    <tr>\n","      <td>10550</td>\n","      <td>0.265100</td>\n","    </tr>\n","    <tr>\n","      <td>10575</td>\n","      <td>0.966900</td>\n","    </tr>\n","    <tr>\n","      <td>10600</td>\n","      <td>0.351000</td>\n","    </tr>\n","    <tr>\n","      <td>10625</td>\n","      <td>1.077900</td>\n","    </tr>\n","    <tr>\n","      <td>10650</td>\n","      <td>0.287600</td>\n","    </tr>\n","    <tr>\n","      <td>10675</td>\n","      <td>1.023700</td>\n","    </tr>\n","    <tr>\n","      <td>10700</td>\n","      <td>0.412200</td>\n","    </tr>\n","    <tr>\n","      <td>10725</td>\n","      <td>1.028800</td>\n","    </tr>\n","    <tr>\n","      <td>10750</td>\n","      <td>0.267000</td>\n","    </tr>\n","    <tr>\n","      <td>10775</td>\n","      <td>1.065700</td>\n","    </tr>\n","    <tr>\n","      <td>10800</td>\n","      <td>0.665300</td>\n","    </tr>\n","    <tr>\n","      <td>10825</td>\n","      <td>1.057900</td>\n","    </tr>\n","    <tr>\n","      <td>10850</td>\n","      <td>0.295200</td>\n","    </tr>\n","    <tr>\n","      <td>10875</td>\n","      <td>0.931700</td>\n","    </tr>\n","    <tr>\n","      <td>10900</td>\n","      <td>0.277000</td>\n","    </tr>\n","    <tr>\n","      <td>10925</td>\n","      <td>0.907400</td>\n","    </tr>\n","    <tr>\n","      <td>10950</td>\n","      <td>0.382100</td>\n","    </tr>\n","    <tr>\n","      <td>10975</td>\n","      <td>0.838500</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.475900</td>\n","    </tr>\n","    <tr>\n","      <td>11025</td>\n","      <td>0.858800</td>\n","    </tr>\n","    <tr>\n","      <td>11050</td>\n","      <td>0.269700</td>\n","    </tr>\n","    <tr>\n","      <td>11075</td>\n","      <td>1.205000</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>0.312800</td>\n","    </tr>\n","    <tr>\n","      <td>11125</td>\n","      <td>1.036200</td>\n","    </tr>\n","    <tr>\n","      <td>11150</td>\n","      <td>0.271000</td>\n","    </tr>\n","    <tr>\n","      <td>11175</td>\n","      <td>1.038200</td>\n","    </tr>\n","    <tr>\n","      <td>11200</td>\n","      <td>0.366200</td>\n","    </tr>\n","    <tr>\n","      <td>11225</td>\n","      <td>0.873900</td>\n","    </tr>\n","    <tr>\n","      <td>11250</td>\n","      <td>0.761400</td>\n","    </tr>\n","    <tr>\n","      <td>11275</td>\n","      <td>1.120900</td>\n","    </tr>\n","    <tr>\n","      <td>11300</td>\n","      <td>0.355400</td>\n","    </tr>\n","    <tr>\n","      <td>11325</td>\n","      <td>1.036400</td>\n","    </tr>\n","    <tr>\n","      <td>11350</td>\n","      <td>0.307600</td>\n","    </tr>\n","    <tr>\n","      <td>11375</td>\n","      <td>1.095200</td>\n","    </tr>\n","    <tr>\n","      <td>11400</td>\n","      <td>0.262000</td>\n","    </tr>\n","    <tr>\n","      <td>11425</td>\n","      <td>0.781300</td>\n","    </tr>\n","    <tr>\n","      <td>11450</td>\n","      <td>0.281600</td>\n","    </tr>\n","    <tr>\n","      <td>11475</td>\n","      <td>1.049900</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.380400</td>\n","    </tr>\n","    <tr>\n","      <td>11525</td>\n","      <td>0.998100</td>\n","    </tr>\n","    <tr>\n","      <td>11550</td>\n","      <td>0.321900</td>\n","    </tr>\n","    <tr>\n","      <td>11575</td>\n","      <td>0.960300</td>\n","    </tr>\n","    <tr>\n","      <td>11600</td>\n","      <td>0.343500</td>\n","    </tr>\n","    <tr>\n","      <td>11625</td>\n","      <td>1.272500</td>\n","    </tr>\n","    <tr>\n","      <td>11650</td>\n","      <td>0.370500</td>\n","    </tr>\n","    <tr>\n","      <td>11675</td>\n","      <td>0.833600</td>\n","    </tr>\n","    <tr>\n","      <td>11700</td>\n","      <td>0.597300</td>\n","    </tr>\n","    <tr>\n","      <td>11725</td>\n","      <td>0.946100</td>\n","    </tr>\n","    <tr>\n","      <td>11750</td>\n","      <td>0.265600</td>\n","    </tr>\n","    <tr>\n","      <td>11775</td>\n","      <td>0.869400</td>\n","    </tr>\n","    <tr>\n","      <td>11800</td>\n","      <td>0.646600</td>\n","    </tr>\n","    <tr>\n","      <td>11825</td>\n","      <td>0.994100</td>\n","    </tr>\n","    <tr>\n","      <td>11850</td>\n","      <td>0.717400</td>\n","    </tr>\n","    <tr>\n","      <td>11875</td>\n","      <td>1.058500</td>\n","    </tr>\n","    <tr>\n","      <td>11900</td>\n","      <td>0.332900</td>\n","    </tr>\n","    <tr>\n","      <td>11925</td>\n","      <td>0.972900</td>\n","    </tr>\n","    <tr>\n","      <td>11950</td>\n","      <td>0.482800</td>\n","    </tr>\n","    <tr>\n","      <td>11975</td>\n","      <td>0.979000</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.345900</td>\n","    </tr>\n","    <tr>\n","      <td>12025</td>\n","      <td>0.970800</td>\n","    </tr>\n","    <tr>\n","      <td>12050</td>\n","      <td>0.370900</td>\n","    </tr>\n","    <tr>\n","      <td>12075</td>\n","      <td>1.006800</td>\n","    </tr>\n","    <tr>\n","      <td>12100</td>\n","      <td>0.431500</td>\n","    </tr>\n","    <tr>\n","      <td>12125</td>\n","      <td>0.928700</td>\n","    </tr>\n","    <tr>\n","      <td>12150</td>\n","      <td>0.441400</td>\n","    </tr>\n","    <tr>\n","      <td>12175</td>\n","      <td>0.984700</td>\n","    </tr>\n","    <tr>\n","      <td>12200</td>\n","      <td>0.513100</td>\n","    </tr>\n","    <tr>\n","      <td>12225</td>\n","      <td>0.993800</td>\n","    </tr>\n","    <tr>\n","      <td>12250</td>\n","      <td>0.365500</td>\n","    </tr>\n","    <tr>\n","      <td>12275</td>\n","      <td>0.957500</td>\n","    </tr>\n","    <tr>\n","      <td>12300</td>\n","      <td>0.458300</td>\n","    </tr>\n","    <tr>\n","      <td>12325</td>\n","      <td>1.062500</td>\n","    </tr>\n","    <tr>\n","      <td>12350</td>\n","      <td>0.378500</td>\n","    </tr>\n","    <tr>\n","      <td>12375</td>\n","      <td>0.984000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=12397, training_loss=0.8017760224714292, metrics={'train_runtime': 28824.6619, 'train_samples_per_second': 1.72, 'train_steps_per_second': 0.43, 'total_flos': 1.2051548481734246e+17, 'train_loss': 0.8017760224714292, 'epoch': 1.0})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Load dataset (you can process it here)\n","dataset = load_dataset('text', data_files=dataset_name, split=\"train\")\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T14:16:47.067160Z","iopub.status.busy":"2024-09-17T14:16:47.066276Z","iopub.status.idle":"2024-09-17T14:16:47.385486Z","shell.execute_reply":"2024-09-17T14:16:47.384641Z","shell.execute_reply.started":"2024-09-17T14:16:47.067116Z"},"trusted":true},"outputs":[],"source":["trainer.model.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T14:17:39.822461Z","iopub.status.busy":"2024-09-17T14:17:39.821677Z","iopub.status.idle":"2024-09-17T14:22:12.972706Z","shell.execute_reply":"2024-09-17T14:22:12.971662Z","shell.execute_reply.started":"2024-09-17T14:17:39.822418Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85a5b261282a4556bb82e5c8e0b025ad","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<s>[INST] I fight with my parents daily..what can I do?? [/INST]  It's common for teenagers to have conflicts with their parents from time to time, but daily arguments can be challenging and emotionally draining. Here are some steps you can take to manage the situation:\n","\n","1. Communicate your feelings: Have an open and honest conversation with your parents about how you're feeling. Let them know what's bothering you and why you're feeling upset. Use \"I\" statements to express your feelings without blaming or attacking them.\n","2. Set boundaries: It's important to set clear boundaries and expectations for how you want to be treated. Let your parents know what you're comfortable with and what you're not.\n","3. Practice active listening: When you're in a conflict with your parents, make sure to listen actively to what they're saying.\n"]}],"source":["# Ignore warnings\n","#logging.set_verbosity(logging.CRITICAL)\n","from transformers import pipeline\n","\n","# Run text generation pipeline with our next model\n","prompt = \"I fight with my parents daily..what can I do??\"\n","pipe = pipeline(task=\"text-generation\", model='/kaggle/working/Llama-finetune-therapy', tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T14:28:36.091875Z","iopub.status.busy":"2024-09-17T14:28:36.091454Z","iopub.status.idle":"2024-09-17T14:30:02.901049Z","shell.execute_reply":"2024-09-17T14:30:02.900172Z","shell.execute_reply.started":"2024-09-17T14:28:36.091837Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"baf4f9f0a3764aa8a658fc377580e223","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  adapters_weights = torch.load(\n"]}],"source":["# Reload model in FP16 and merge it with LoRA weights\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T14:30:37.743083Z","iopub.status.busy":"2024-09-17T14:30:37.742006Z","iopub.status.idle":"2024-09-17T14:30:37.748210Z","shell.execute_reply":"2024-09-17T14:30:37.747158Z","shell.execute_reply.started":"2024-09-17T14:30:37.743025Z"},"trusted":true},"outputs":[],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\""]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-17T14:39:41.191631Z","iopub.status.busy":"2024-09-17T14:39:41.190953Z","iopub.status.idle":"2024-09-17T14:43:55.773384Z","shell.execute_reply":"2024-09-17T14:43:55.772232Z","shell.execute_reply.started":"2024-09-17T14:39:41.191588Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:834: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d7d62a320cf47dbbcc206c50c1ec2dd","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8c082db9b9a4c368d187d6e503e1c98","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38092c6f5cfb4314974e7781b6ad43bc","version_major":2,"version_minor":0},"text/plain":["Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0cacec5b4ee24d12a3c32c73b4b687d3","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:834: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b546ce2b102e4dbd93a858d27dce2a14","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"688efd7b8c6e460e9c5014e5a99a930e","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/thrishala/mental_health_chatbot/commit/ec3434ede01390b5f580b939085e6c8cc5caddfb', commit_message='Upload tokenizer', commit_description='', oid='ec3434ede01390b5f580b939085e6c8cc5caddfb', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["!huggingface-cli login --token your_access_token\n","\n","model.push_to_hub(\"thrishala/mental_health_chatbot\", use_auth_token=\"your_access_token\", check_pr=True)\n","\n","tokenizer.push_to_hub(\"thrishala/mental_health_chatbot\", use_auth_token=\"your_access_token\", check_pr=True)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5614265,"sourceId":9276202,"sourceType":"datasetVersion"},{"datasetId":5614274,"sourceId":9276214,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
