{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9276202,"sourceType":"datasetVersion","datasetId":5614265},{"sourceId":9276214,"sourceType":"datasetVersion","datasetId":5614274}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile dataformat1.py\n\nimport json\n\n# Path to the input JSON file\ninput_file = '/kaggle/input/dataset1/Alexander_Street_shareGPT_2.0.json'\n\n# Path to the output text file\noutput_file = '/kaggle/working/data1'\n# Load the JSON data\nwith open(input_file, 'r') as f:\n    data = json.load(f)\n\n# Open the output file in write mode\nwith open(output_file, 'w') as f:\n    for entry in data:\n        instruction = entry['instruction']\n        user_input = entry['input']\n        output = entry['output']\n        \n        # Construct the formatted string\n        formatted = (\n            f\"<s>[INST] <<SYS>>\\n\"\n            f\"{instruction}\\n\"\n            f\"<</SYS>>\\n\"\n            f\"{user_input} [/INST] {output} </s>\\n\"\n        )\n        \n        # Write to the output file\n        f.write(formatted)\n\nprint(\"Conversion complete! Output saved to:\", output_file)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:12:01.094466Z","iopub.execute_input":"2024-09-17T06:12:01.094778Z","iopub.status.idle":"2024-09-17T06:12:01.106962Z","shell.execute_reply.started":"2024-09-17T06:12:01.094743Z","shell.execute_reply":"2024-09-17T06:12:01.106071Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Writing dataformat1.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python dataformat1.py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-17T06:12:01.108420Z","iopub.execute_input":"2024-09-17T06:12:01.108745Z","iopub.status.idle":"2024-09-17T06:12:02.401738Z","shell.execute_reply.started":"2024-09-17T06:12:01.108712Z","shell.execute_reply":"2024-09-17T06:12:02.400623Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Conversion complete! Output saved to: /kaggle/working/data1\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile dataformat2.py\n\nimport json\n\n# Path to the input JSON file\ninput_file = '/kaggle/input/dataset2/amod-dataset.json'\n\n# Path to the output text file\noutput_file = '/kaggle/working/data2'\n\n# Load the JSON data\nwith open(input_file, 'r') as f:\n    data = f.readlines()\n\n# Open the output file in write mode\nwith open(output_file, 'w') as f:\n    for line in data:\n        entry = json.loads(line)\n        context = entry['Context'].replace(\"\\u00a0\", \"\")\n        response = entry['Response'].replace(\"\\u00a0\", \"\")\n        instruction = \"If you are a counsellor, please answer the questions based on the description of the patient.\"\n        \n        # Construct the formatted string\n        formatted = (\n            f\"<s>[INST] <<SYS>>\\n\"\n            f\"{instruction}\\n\"\n            f\"<</SYS>>\\n\"\n            f\"{context} [/INST] {response} </s>\\n\"\n        )\n        \n        # Write to the output file\n        f.write(formatted)\n\nprint(\"Conversion complete! Output saved to:\", output_file)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:12:05.521119Z","iopub.execute_input":"2024-09-17T06:12:05.521537Z","iopub.status.idle":"2024-09-17T06:12:05.531684Z","shell.execute_reply.started":"2024-09-17T06:12:05.521497Z","shell.execute_reply":"2024-09-17T06:12:05.530459Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing dataformat2.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python dataformat2.py","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:12:09.175186Z","iopub.execute_input":"2024-09-17T06:12:09.175787Z","iopub.status.idle":"2024-09-17T06:12:10.495387Z","shell.execute_reply.started":"2024-09-17T06:12:09.175726Z","shell.execute_reply":"2024-09-17T06:12:10.494420Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Conversion complete! Output saved to: /kaggle/working/data2\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile combineddata.py\n\n# Paths to the input files\ninput_file1 = '/kaggle/working/data1'\ninput_file2 = '/kaggle/working/data2'\n\n# Path to the output file where both files will be combined\noutput_file = '/kaggle/working/finaldata'\n# Open the output file in write mode\nwith open(output_file, 'w') as outfile:\n    # Append the content of the first file\n    with open(input_file1, 'r') as infile1:\n        outfile.write(infile1.read())\n    \n    # Append the content of the second file\n    with open(input_file2, 'r') as infile2:\n        outfile.write(infile2.read())\n\nprint(\"Files have been successfully combined into:\", output_file)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:12:17.909930Z","iopub.execute_input":"2024-09-17T06:12:17.910791Z","iopub.status.idle":"2024-09-17T06:12:17.916426Z","shell.execute_reply.started":"2024-09-17T06:12:17.910749Z","shell.execute_reply":"2024-09-17T06:12:17.915401Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Overwriting combineddata.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python /kaggle/working/combineddata.py","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:12:21.603162Z","iopub.execute_input":"2024-09-17T06:12:21.603953Z","iopub.status.idle":"2024-09-17T06:12:22.777232Z","shell.execute_reply.started":"2024-09-17T06:12:21.603911Z","shell.execute_reply":"2024-09-17T06:12:22.776244Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Files have been successfully combined into: /kaggle/working/finaldata\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q accelerate==0.34.2 peft==0.4.0 bitsandbytes==0.43.3 transformers==4.38.2 trl==0.4.7","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:12:25.634240Z","iopub.execute_input":"2024-09-17T06:12:25.635158Z","iopub.status.idle":"2024-09-17T06:12:57.405900Z","shell.execute_reply.started":"2024-09-17T06:12:25.635115Z","shell.execute_reply":"2024-09-17T06:12:57.404683Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:27:36.840434Z","iopub.execute_input":"2024-09-17T14:27:36.841218Z","iopub.status.idle":"2024-09-17T14:27:51.723541Z","shell.execute_reply.started":"2024-09-17T14:27:36.841177Z","shell.execute_reply":"2024-09-17T14:27:51.722420Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"/kaggle/working/finaldata\"\n\n# Fine-tuned model name\nnew_model = \"Llama-finetune-therapy\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"/kaggle/working/\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:28:26.012489Z","iopub.execute_input":"2024-09-17T14:28:26.013734Z","iopub.status.idle":"2024-09-17T14:28:26.025383Z","shell.execute_reply.started":"2024-09-17T14:28:26.013687Z","shell.execute_reply":"2024-09-17T14:28:26.024276Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load dataset (you can process it here)\ndataset = load_dataset('text', data_files=dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:13:46.940145Z","iopub.execute_input":"2024-09-17T06:13:46.940548Z","iopub.status.idle":"2024-09-17T14:15:41.408758Z","shell.execute_reply.started":"2024-09-17T06:13:46.940509Z","shell.execute_reply":"2024-09-17T14:15:41.407821Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"373cde8e8755406b826840234b18aba2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bf61e4eb4dc4bbdb728bf3b882aca81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4f1e1eba054e829596c1c95f74fd48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"596ba32a7d5747cc8a8cae7aabc5ba72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f68531fdf0479e84970eb751814133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edc3d250b2f04653b173f9fdf1a6add7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3109646690640c796442f8c04f60196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f17f65dd861b4ccfbfc349d928deb738"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52efbcc4c42c4ec3ae3a869284847949"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6f0ee6721e34ff798fbed978ea52cc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe990987c9f410aafb4b813c28a5646"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"911d0d7302424277bffdd40a6f74f5ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e94b5a600844268a1655bb12e5c18f6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/49586 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66dd2904677e444d89d6bd0ce8268042"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12397' max='12397' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12397/12397 7:59:59, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.835600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>9.529900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>2.630100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.257400</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.968600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.310800</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.148300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.735700</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.223000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.955000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.298800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.525000</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.108000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.164000</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.075100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.456300</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.961300</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.863100</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.185600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.856200</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.242700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.714200</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>1.055400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.787800</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.999300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.975800</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>1.308100</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.641700</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>1.461700</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.973600</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>1.191500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.834600</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>1.049700</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.985700</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>1.210400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.745800</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>1.077500</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.486100</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>1.193900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.846800</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>1.009900</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.806800</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>1.061800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.806000</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>0.982400</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.820500</td>\n    </tr>\n    <tr>\n      <td>1175</td>\n      <td>1.053100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.613800</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>1.152300</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.761000</td>\n    </tr>\n    <tr>\n      <td>1275</td>\n      <td>0.818400</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.836500</td>\n    </tr>\n    <tr>\n      <td>1325</td>\n      <td>1.159600</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.611200</td>\n    </tr>\n    <tr>\n      <td>1375</td>\n      <td>1.026400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.899900</td>\n    </tr>\n    <tr>\n      <td>1425</td>\n      <td>1.176400</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.594000</td>\n    </tr>\n    <tr>\n      <td>1475</td>\n      <td>1.195000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.524000</td>\n    </tr>\n    <tr>\n      <td>1525</td>\n      <td>1.131200</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.513900</td>\n    </tr>\n    <tr>\n      <td>1575</td>\n      <td>1.092800</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.509000</td>\n    </tr>\n    <tr>\n      <td>1625</td>\n      <td>1.194700</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.696100</td>\n    </tr>\n    <tr>\n      <td>1675</td>\n      <td>1.251200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.310300</td>\n    </tr>\n    <tr>\n      <td>1725</td>\n      <td>1.080300</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.462500</td>\n    </tr>\n    <tr>\n      <td>1775</td>\n      <td>0.916200</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.707300</td>\n    </tr>\n    <tr>\n      <td>1825</td>\n      <td>1.167600</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.572300</td>\n    </tr>\n    <tr>\n      <td>1875</td>\n      <td>0.989400</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.628200</td>\n    </tr>\n    <tr>\n      <td>1925</td>\n      <td>1.091000</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.420800</td>\n    </tr>\n    <tr>\n      <td>1975</td>\n      <td>0.872100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.680300</td>\n    </tr>\n    <tr>\n      <td>2025</td>\n      <td>1.213800</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.647700</td>\n    </tr>\n    <tr>\n      <td>2075</td>\n      <td>1.059000</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.618000</td>\n    </tr>\n    <tr>\n      <td>2125</td>\n      <td>1.137700</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.340700</td>\n    </tr>\n    <tr>\n      <td>2175</td>\n      <td>1.286800</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.428900</td>\n    </tr>\n    <tr>\n      <td>2225</td>\n      <td>1.308500</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.347600</td>\n    </tr>\n    <tr>\n      <td>2275</td>\n      <td>1.012300</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.586500</td>\n    </tr>\n    <tr>\n      <td>2325</td>\n      <td>1.170200</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.435200</td>\n    </tr>\n    <tr>\n      <td>2375</td>\n      <td>1.311200</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.644500</td>\n    </tr>\n    <tr>\n      <td>2425</td>\n      <td>1.246400</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.519100</td>\n    </tr>\n    <tr>\n      <td>2475</td>\n      <td>1.079700</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.495400</td>\n    </tr>\n    <tr>\n      <td>2525</td>\n      <td>1.163400</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.543600</td>\n    </tr>\n    <tr>\n      <td>2575</td>\n      <td>1.032400</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.486000</td>\n    </tr>\n    <tr>\n      <td>2625</td>\n      <td>0.977400</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.571700</td>\n    </tr>\n    <tr>\n      <td>2675</td>\n      <td>1.208500</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.464300</td>\n    </tr>\n    <tr>\n      <td>2725</td>\n      <td>1.049400</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.557400</td>\n    </tr>\n    <tr>\n      <td>2775</td>\n      <td>1.079300</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.680000</td>\n    </tr>\n    <tr>\n      <td>2825</td>\n      <td>1.013800</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.434200</td>\n    </tr>\n    <tr>\n      <td>2875</td>\n      <td>1.089000</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.393000</td>\n    </tr>\n    <tr>\n      <td>2925</td>\n      <td>0.987600</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.558600</td>\n    </tr>\n    <tr>\n      <td>2975</td>\n      <td>1.352300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.660700</td>\n    </tr>\n    <tr>\n      <td>3025</td>\n      <td>1.124700</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.446700</td>\n    </tr>\n    <tr>\n      <td>3075</td>\n      <td>0.994500</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.642800</td>\n    </tr>\n    <tr>\n      <td>3125</td>\n      <td>0.990100</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.421800</td>\n    </tr>\n    <tr>\n      <td>3175</td>\n      <td>0.737400</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.435900</td>\n    </tr>\n    <tr>\n      <td>3225</td>\n      <td>1.096100</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.447000</td>\n    </tr>\n    <tr>\n      <td>3275</td>\n      <td>0.972000</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.562400</td>\n    </tr>\n    <tr>\n      <td>3325</td>\n      <td>1.023400</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.388800</td>\n    </tr>\n    <tr>\n      <td>3375</td>\n      <td>1.131000</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.477800</td>\n    </tr>\n    <tr>\n      <td>3425</td>\n      <td>0.983400</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.407700</td>\n    </tr>\n    <tr>\n      <td>3475</td>\n      <td>0.934000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.523100</td>\n    </tr>\n    <tr>\n      <td>3525</td>\n      <td>0.931200</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.555000</td>\n    </tr>\n    <tr>\n      <td>3575</td>\n      <td>1.026000</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.448300</td>\n    </tr>\n    <tr>\n      <td>3625</td>\n      <td>1.061500</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.473000</td>\n    </tr>\n    <tr>\n      <td>3675</td>\n      <td>1.175100</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.721400</td>\n    </tr>\n    <tr>\n      <td>3725</td>\n      <td>1.050200</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.391200</td>\n    </tr>\n    <tr>\n      <td>3775</td>\n      <td>0.969600</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.556700</td>\n    </tr>\n    <tr>\n      <td>3825</td>\n      <td>1.189700</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.550000</td>\n    </tr>\n    <tr>\n      <td>3875</td>\n      <td>0.907800</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.337000</td>\n    </tr>\n    <tr>\n      <td>3925</td>\n      <td>0.956600</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.375900</td>\n    </tr>\n    <tr>\n      <td>3975</td>\n      <td>1.206900</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.443000</td>\n    </tr>\n    <tr>\n      <td>4025</td>\n      <td>1.129000</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.423400</td>\n    </tr>\n    <tr>\n      <td>4075</td>\n      <td>0.982300</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.417000</td>\n    </tr>\n    <tr>\n      <td>4125</td>\n      <td>0.987400</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.545200</td>\n    </tr>\n    <tr>\n      <td>4175</td>\n      <td>1.161000</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.506400</td>\n    </tr>\n    <tr>\n      <td>4225</td>\n      <td>1.208300</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.343600</td>\n    </tr>\n    <tr>\n      <td>4275</td>\n      <td>1.211700</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.316200</td>\n    </tr>\n    <tr>\n      <td>4325</td>\n      <td>1.089700</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.372900</td>\n    </tr>\n    <tr>\n      <td>4375</td>\n      <td>1.299000</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.406000</td>\n    </tr>\n    <tr>\n      <td>4425</td>\n      <td>1.053300</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.334200</td>\n    </tr>\n    <tr>\n      <td>4475</td>\n      <td>1.028600</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.388900</td>\n    </tr>\n    <tr>\n      <td>4525</td>\n      <td>1.039700</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.408000</td>\n    </tr>\n    <tr>\n      <td>4575</td>\n      <td>0.895500</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.327800</td>\n    </tr>\n    <tr>\n      <td>4625</td>\n      <td>1.067500</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.327700</td>\n    </tr>\n    <tr>\n      <td>4675</td>\n      <td>0.993500</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.405900</td>\n    </tr>\n    <tr>\n      <td>4725</td>\n      <td>1.155900</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.595800</td>\n    </tr>\n    <tr>\n      <td>4775</td>\n      <td>0.927100</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.444600</td>\n    </tr>\n    <tr>\n      <td>4825</td>\n      <td>1.255000</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.307000</td>\n    </tr>\n    <tr>\n      <td>4875</td>\n      <td>1.215100</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.372400</td>\n    </tr>\n    <tr>\n      <td>4925</td>\n      <td>1.075100</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.438400</td>\n    </tr>\n    <tr>\n      <td>4975</td>\n      <td>0.878100</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.440600</td>\n    </tr>\n    <tr>\n      <td>5025</td>\n      <td>0.973200</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.578700</td>\n    </tr>\n    <tr>\n      <td>5075</td>\n      <td>0.923700</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.553700</td>\n    </tr>\n    <tr>\n      <td>5125</td>\n      <td>1.015600</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.732200</td>\n    </tr>\n    <tr>\n      <td>5175</td>\n      <td>0.980100</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.475600</td>\n    </tr>\n    <tr>\n      <td>5225</td>\n      <td>1.033700</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.470900</td>\n    </tr>\n    <tr>\n      <td>5275</td>\n      <td>0.966500</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.530100</td>\n    </tr>\n    <tr>\n      <td>5325</td>\n      <td>0.955300</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>0.363600</td>\n    </tr>\n    <tr>\n      <td>5375</td>\n      <td>1.003600</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.376500</td>\n    </tr>\n    <tr>\n      <td>5425</td>\n      <td>0.974300</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>0.364300</td>\n    </tr>\n    <tr>\n      <td>5475</td>\n      <td>0.959100</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.472700</td>\n    </tr>\n    <tr>\n      <td>5525</td>\n      <td>0.917100</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>0.593800</td>\n    </tr>\n    <tr>\n      <td>5575</td>\n      <td>1.129700</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.424300</td>\n    </tr>\n    <tr>\n      <td>5625</td>\n      <td>0.906300</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>0.431800</td>\n    </tr>\n    <tr>\n      <td>5675</td>\n      <td>1.016700</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.303300</td>\n    </tr>\n    <tr>\n      <td>5725</td>\n      <td>1.251700</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>0.337900</td>\n    </tr>\n    <tr>\n      <td>5775</td>\n      <td>0.991000</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.292600</td>\n    </tr>\n    <tr>\n      <td>5825</td>\n      <td>1.217600</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>0.376100</td>\n    </tr>\n    <tr>\n      <td>5875</td>\n      <td>1.109900</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.310900</td>\n    </tr>\n    <tr>\n      <td>5925</td>\n      <td>1.131300</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>0.679400</td>\n    </tr>\n    <tr>\n      <td>5975</td>\n      <td>1.078700</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.308400</td>\n    </tr>\n    <tr>\n      <td>6025</td>\n      <td>1.191200</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>0.301900</td>\n    </tr>\n    <tr>\n      <td>6075</td>\n      <td>1.081700</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.536100</td>\n    </tr>\n    <tr>\n      <td>6125</td>\n      <td>1.057600</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>0.346300</td>\n    </tr>\n    <tr>\n      <td>6175</td>\n      <td>0.907200</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.479700</td>\n    </tr>\n    <tr>\n      <td>6225</td>\n      <td>1.036500</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>0.374100</td>\n    </tr>\n    <tr>\n      <td>6275</td>\n      <td>1.037700</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.315800</td>\n    </tr>\n    <tr>\n      <td>6325</td>\n      <td>1.259100</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>0.345900</td>\n    </tr>\n    <tr>\n      <td>6375</td>\n      <td>0.927200</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.429300</td>\n    </tr>\n    <tr>\n      <td>6425</td>\n      <td>0.996900</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>0.622000</td>\n    </tr>\n    <tr>\n      <td>6475</td>\n      <td>0.947100</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.468800</td>\n    </tr>\n    <tr>\n      <td>6525</td>\n      <td>0.946000</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>0.297700</td>\n    </tr>\n    <tr>\n      <td>6575</td>\n      <td>1.001700</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.494400</td>\n    </tr>\n    <tr>\n      <td>6625</td>\n      <td>1.126200</td>\n    </tr>\n    <tr>\n      <td>6650</td>\n      <td>0.403500</td>\n    </tr>\n    <tr>\n      <td>6675</td>\n      <td>1.178700</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.321800</td>\n    </tr>\n    <tr>\n      <td>6725</td>\n      <td>1.241000</td>\n    </tr>\n    <tr>\n      <td>6750</td>\n      <td>0.384300</td>\n    </tr>\n    <tr>\n      <td>6775</td>\n      <td>1.009000</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.296800</td>\n    </tr>\n    <tr>\n      <td>6825</td>\n      <td>0.853300</td>\n    </tr>\n    <tr>\n      <td>6850</td>\n      <td>0.582200</td>\n    </tr>\n    <tr>\n      <td>6875</td>\n      <td>0.975900</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.390700</td>\n    </tr>\n    <tr>\n      <td>6925</td>\n      <td>1.267300</td>\n    </tr>\n    <tr>\n      <td>6950</td>\n      <td>0.314900</td>\n    </tr>\n    <tr>\n      <td>6975</td>\n      <td>0.908000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.451600</td>\n    </tr>\n    <tr>\n      <td>7025</td>\n      <td>0.974900</td>\n    </tr>\n    <tr>\n      <td>7050</td>\n      <td>0.372100</td>\n    </tr>\n    <tr>\n      <td>7075</td>\n      <td>1.105600</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.365300</td>\n    </tr>\n    <tr>\n      <td>7125</td>\n      <td>0.955900</td>\n    </tr>\n    <tr>\n      <td>7150</td>\n      <td>0.533000</td>\n    </tr>\n    <tr>\n      <td>7175</td>\n      <td>0.904700</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.566800</td>\n    </tr>\n    <tr>\n      <td>7225</td>\n      <td>1.076800</td>\n    </tr>\n    <tr>\n      <td>7250</td>\n      <td>0.283000</td>\n    </tr>\n    <tr>\n      <td>7275</td>\n      <td>1.161000</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.291200</td>\n    </tr>\n    <tr>\n      <td>7325</td>\n      <td>1.196100</td>\n    </tr>\n    <tr>\n      <td>7350</td>\n      <td>0.331100</td>\n    </tr>\n    <tr>\n      <td>7375</td>\n      <td>1.175500</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.322400</td>\n    </tr>\n    <tr>\n      <td>7425</td>\n      <td>0.886400</td>\n    </tr>\n    <tr>\n      <td>7450</td>\n      <td>0.306000</td>\n    </tr>\n    <tr>\n      <td>7475</td>\n      <td>1.094400</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.451200</td>\n    </tr>\n    <tr>\n      <td>7525</td>\n      <td>0.997100</td>\n    </tr>\n    <tr>\n      <td>7550</td>\n      <td>0.284900</td>\n    </tr>\n    <tr>\n      <td>7575</td>\n      <td>1.123200</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.432900</td>\n    </tr>\n    <tr>\n      <td>7625</td>\n      <td>1.104700</td>\n    </tr>\n    <tr>\n      <td>7650</td>\n      <td>0.286800</td>\n    </tr>\n    <tr>\n      <td>7675</td>\n      <td>1.075600</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>0.293700</td>\n    </tr>\n    <tr>\n      <td>7725</td>\n      <td>0.980500</td>\n    </tr>\n    <tr>\n      <td>7750</td>\n      <td>0.513100</td>\n    </tr>\n    <tr>\n      <td>7775</td>\n      <td>0.869300</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.390000</td>\n    </tr>\n    <tr>\n      <td>7825</td>\n      <td>1.102100</td>\n    </tr>\n    <tr>\n      <td>7850</td>\n      <td>0.285000</td>\n    </tr>\n    <tr>\n      <td>7875</td>\n      <td>0.890300</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>0.424100</td>\n    </tr>\n    <tr>\n      <td>7925</td>\n      <td>0.925900</td>\n    </tr>\n    <tr>\n      <td>7950</td>\n      <td>0.298900</td>\n    </tr>\n    <tr>\n      <td>7975</td>\n      <td>0.981700</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.395600</td>\n    </tr>\n    <tr>\n      <td>8025</td>\n      <td>1.234200</td>\n    </tr>\n    <tr>\n      <td>8050</td>\n      <td>0.291300</td>\n    </tr>\n    <tr>\n      <td>8075</td>\n      <td>0.886200</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.475400</td>\n    </tr>\n    <tr>\n      <td>8125</td>\n      <td>0.916800</td>\n    </tr>\n    <tr>\n      <td>8150</td>\n      <td>0.477800</td>\n    </tr>\n    <tr>\n      <td>8175</td>\n      <td>0.918900</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.380000</td>\n    </tr>\n    <tr>\n      <td>8225</td>\n      <td>1.085000</td>\n    </tr>\n    <tr>\n      <td>8250</td>\n      <td>0.611100</td>\n    </tr>\n    <tr>\n      <td>8275</td>\n      <td>0.934500</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.673000</td>\n    </tr>\n    <tr>\n      <td>8325</td>\n      <td>0.812000</td>\n    </tr>\n    <tr>\n      <td>8350</td>\n      <td>0.489700</td>\n    </tr>\n    <tr>\n      <td>8375</td>\n      <td>0.971400</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.486900</td>\n    </tr>\n    <tr>\n      <td>8425</td>\n      <td>0.903300</td>\n    </tr>\n    <tr>\n      <td>8450</td>\n      <td>0.351700</td>\n    </tr>\n    <tr>\n      <td>8475</td>\n      <td>1.067500</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.423100</td>\n    </tr>\n    <tr>\n      <td>8525</td>\n      <td>0.915800</td>\n    </tr>\n    <tr>\n      <td>8550</td>\n      <td>0.419300</td>\n    </tr>\n    <tr>\n      <td>8575</td>\n      <td>0.869500</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.557800</td>\n    </tr>\n    <tr>\n      <td>8625</td>\n      <td>1.104700</td>\n    </tr>\n    <tr>\n      <td>8650</td>\n      <td>0.270600</td>\n    </tr>\n    <tr>\n      <td>8675</td>\n      <td>1.005900</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.448300</td>\n    </tr>\n    <tr>\n      <td>8725</td>\n      <td>0.909900</td>\n    </tr>\n    <tr>\n      <td>8750</td>\n      <td>0.424300</td>\n    </tr>\n    <tr>\n      <td>8775</td>\n      <td>1.116200</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.407900</td>\n    </tr>\n    <tr>\n      <td>8825</td>\n      <td>1.074100</td>\n    </tr>\n    <tr>\n      <td>8850</td>\n      <td>0.420900</td>\n    </tr>\n    <tr>\n      <td>8875</td>\n      <td>0.926000</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.287100</td>\n    </tr>\n    <tr>\n      <td>8925</td>\n      <td>1.018700</td>\n    </tr>\n    <tr>\n      <td>8950</td>\n      <td>0.268400</td>\n    </tr>\n    <tr>\n      <td>8975</td>\n      <td>0.963100</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.384200</td>\n    </tr>\n    <tr>\n      <td>9025</td>\n      <td>0.925900</td>\n    </tr>\n    <tr>\n      <td>9050</td>\n      <td>0.611500</td>\n    </tr>\n    <tr>\n      <td>9075</td>\n      <td>1.114200</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>0.308200</td>\n    </tr>\n    <tr>\n      <td>9125</td>\n      <td>0.933300</td>\n    </tr>\n    <tr>\n      <td>9150</td>\n      <td>0.285800</td>\n    </tr>\n    <tr>\n      <td>9175</td>\n      <td>0.933300</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.697100</td>\n    </tr>\n    <tr>\n      <td>9225</td>\n      <td>0.969500</td>\n    </tr>\n    <tr>\n      <td>9250</td>\n      <td>0.490400</td>\n    </tr>\n    <tr>\n      <td>9275</td>\n      <td>1.119900</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>0.374500</td>\n    </tr>\n    <tr>\n      <td>9325</td>\n      <td>0.967900</td>\n    </tr>\n    <tr>\n      <td>9350</td>\n      <td>0.420200</td>\n    </tr>\n    <tr>\n      <td>9375</td>\n      <td>1.015100</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>0.281200</td>\n    </tr>\n    <tr>\n      <td>9425</td>\n      <td>1.004500</td>\n    </tr>\n    <tr>\n      <td>9450</td>\n      <td>0.436800</td>\n    </tr>\n    <tr>\n      <td>9475</td>\n      <td>1.066500</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.264700</td>\n    </tr>\n    <tr>\n      <td>9525</td>\n      <td>1.132600</td>\n    </tr>\n    <tr>\n      <td>9550</td>\n      <td>0.318000</td>\n    </tr>\n    <tr>\n      <td>9575</td>\n      <td>0.979500</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>0.269500</td>\n    </tr>\n    <tr>\n      <td>9625</td>\n      <td>0.982700</td>\n    </tr>\n    <tr>\n      <td>9650</td>\n      <td>0.368000</td>\n    </tr>\n    <tr>\n      <td>9675</td>\n      <td>0.742100</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>0.296500</td>\n    </tr>\n    <tr>\n      <td>9725</td>\n      <td>1.116000</td>\n    </tr>\n    <tr>\n      <td>9750</td>\n      <td>0.381200</td>\n    </tr>\n    <tr>\n      <td>9775</td>\n      <td>1.042600</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>0.739100</td>\n    </tr>\n    <tr>\n      <td>9825</td>\n      <td>1.075300</td>\n    </tr>\n    <tr>\n      <td>9850</td>\n      <td>0.272400</td>\n    </tr>\n    <tr>\n      <td>9875</td>\n      <td>0.920000</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>0.468400</td>\n    </tr>\n    <tr>\n      <td>9925</td>\n      <td>1.228700</td>\n    </tr>\n    <tr>\n      <td>9950</td>\n      <td>0.276200</td>\n    </tr>\n    <tr>\n      <td>9975</td>\n      <td>0.969500</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.334300</td>\n    </tr>\n    <tr>\n      <td>10025</td>\n      <td>0.974400</td>\n    </tr>\n    <tr>\n      <td>10050</td>\n      <td>0.321500</td>\n    </tr>\n    <tr>\n      <td>10075</td>\n      <td>0.969400</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>0.465200</td>\n    </tr>\n    <tr>\n      <td>10125</td>\n      <td>1.082200</td>\n    </tr>\n    <tr>\n      <td>10150</td>\n      <td>0.281400</td>\n    </tr>\n    <tr>\n      <td>10175</td>\n      <td>1.045600</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>0.358700</td>\n    </tr>\n    <tr>\n      <td>10225</td>\n      <td>0.853500</td>\n    </tr>\n    <tr>\n      <td>10250</td>\n      <td>0.420600</td>\n    </tr>\n    <tr>\n      <td>10275</td>\n      <td>0.980400</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>0.502700</td>\n    </tr>\n    <tr>\n      <td>10325</td>\n      <td>1.122500</td>\n    </tr>\n    <tr>\n      <td>10350</td>\n      <td>0.342700</td>\n    </tr>\n    <tr>\n      <td>10375</td>\n      <td>0.882700</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>0.263300</td>\n    </tr>\n    <tr>\n      <td>10425</td>\n      <td>1.063700</td>\n    </tr>\n    <tr>\n      <td>10450</td>\n      <td>0.310400</td>\n    </tr>\n    <tr>\n      <td>10475</td>\n      <td>1.187300</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.411500</td>\n    </tr>\n    <tr>\n      <td>10525</td>\n      <td>0.987700</td>\n    </tr>\n    <tr>\n      <td>10550</td>\n      <td>0.265100</td>\n    </tr>\n    <tr>\n      <td>10575</td>\n      <td>0.966900</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>0.351000</td>\n    </tr>\n    <tr>\n      <td>10625</td>\n      <td>1.077900</td>\n    </tr>\n    <tr>\n      <td>10650</td>\n      <td>0.287600</td>\n    </tr>\n    <tr>\n      <td>10675</td>\n      <td>1.023700</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>0.412200</td>\n    </tr>\n    <tr>\n      <td>10725</td>\n      <td>1.028800</td>\n    </tr>\n    <tr>\n      <td>10750</td>\n      <td>0.267000</td>\n    </tr>\n    <tr>\n      <td>10775</td>\n      <td>1.065700</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>0.665300</td>\n    </tr>\n    <tr>\n      <td>10825</td>\n      <td>1.057900</td>\n    </tr>\n    <tr>\n      <td>10850</td>\n      <td>0.295200</td>\n    </tr>\n    <tr>\n      <td>10875</td>\n      <td>0.931700</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>0.277000</td>\n    </tr>\n    <tr>\n      <td>10925</td>\n      <td>0.907400</td>\n    </tr>\n    <tr>\n      <td>10950</td>\n      <td>0.382100</td>\n    </tr>\n    <tr>\n      <td>10975</td>\n      <td>0.838500</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.475900</td>\n    </tr>\n    <tr>\n      <td>11025</td>\n      <td>0.858800</td>\n    </tr>\n    <tr>\n      <td>11050</td>\n      <td>0.269700</td>\n    </tr>\n    <tr>\n      <td>11075</td>\n      <td>1.205000</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>0.312800</td>\n    </tr>\n    <tr>\n      <td>11125</td>\n      <td>1.036200</td>\n    </tr>\n    <tr>\n      <td>11150</td>\n      <td>0.271000</td>\n    </tr>\n    <tr>\n      <td>11175</td>\n      <td>1.038200</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>0.366200</td>\n    </tr>\n    <tr>\n      <td>11225</td>\n      <td>0.873900</td>\n    </tr>\n    <tr>\n      <td>11250</td>\n      <td>0.761400</td>\n    </tr>\n    <tr>\n      <td>11275</td>\n      <td>1.120900</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>0.355400</td>\n    </tr>\n    <tr>\n      <td>11325</td>\n      <td>1.036400</td>\n    </tr>\n    <tr>\n      <td>11350</td>\n      <td>0.307600</td>\n    </tr>\n    <tr>\n      <td>11375</td>\n      <td>1.095200</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>0.262000</td>\n    </tr>\n    <tr>\n      <td>11425</td>\n      <td>0.781300</td>\n    </tr>\n    <tr>\n      <td>11450</td>\n      <td>0.281600</td>\n    </tr>\n    <tr>\n      <td>11475</td>\n      <td>1.049900</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.380400</td>\n    </tr>\n    <tr>\n      <td>11525</td>\n      <td>0.998100</td>\n    </tr>\n    <tr>\n      <td>11550</td>\n      <td>0.321900</td>\n    </tr>\n    <tr>\n      <td>11575</td>\n      <td>0.960300</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>0.343500</td>\n    </tr>\n    <tr>\n      <td>11625</td>\n      <td>1.272500</td>\n    </tr>\n    <tr>\n      <td>11650</td>\n      <td>0.370500</td>\n    </tr>\n    <tr>\n      <td>11675</td>\n      <td>0.833600</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>0.597300</td>\n    </tr>\n    <tr>\n      <td>11725</td>\n      <td>0.946100</td>\n    </tr>\n    <tr>\n      <td>11750</td>\n      <td>0.265600</td>\n    </tr>\n    <tr>\n      <td>11775</td>\n      <td>0.869400</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>0.646600</td>\n    </tr>\n    <tr>\n      <td>11825</td>\n      <td>0.994100</td>\n    </tr>\n    <tr>\n      <td>11850</td>\n      <td>0.717400</td>\n    </tr>\n    <tr>\n      <td>11875</td>\n      <td>1.058500</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>0.332900</td>\n    </tr>\n    <tr>\n      <td>11925</td>\n      <td>0.972900</td>\n    </tr>\n    <tr>\n      <td>11950</td>\n      <td>0.482800</td>\n    </tr>\n    <tr>\n      <td>11975</td>\n      <td>0.979000</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.345900</td>\n    </tr>\n    <tr>\n      <td>12025</td>\n      <td>0.970800</td>\n    </tr>\n    <tr>\n      <td>12050</td>\n      <td>0.370900</td>\n    </tr>\n    <tr>\n      <td>12075</td>\n      <td>1.006800</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>0.431500</td>\n    </tr>\n    <tr>\n      <td>12125</td>\n      <td>0.928700</td>\n    </tr>\n    <tr>\n      <td>12150</td>\n      <td>0.441400</td>\n    </tr>\n    <tr>\n      <td>12175</td>\n      <td>0.984700</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>0.513100</td>\n    </tr>\n    <tr>\n      <td>12225</td>\n      <td>0.993800</td>\n    </tr>\n    <tr>\n      <td>12250</td>\n      <td>0.365500</td>\n    </tr>\n    <tr>\n      <td>12275</td>\n      <td>0.957500</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>0.458300</td>\n    </tr>\n    <tr>\n      <td>12325</td>\n      <td>1.062500</td>\n    </tr>\n    <tr>\n      <td>12350</td>\n      <td>0.378500</td>\n    </tr>\n    <tr>\n      <td>12375</td>\n      <td>0.984000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=12397, training_loss=0.8017760224714292, metrics={'train_runtime': 28824.6619, 'train_samples_per_second': 1.72, 'train_steps_per_second': 0.43, 'total_flos': 1.2051548481734246e+17, 'train_loss': 0.8017760224714292, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:16:47.066276Z","iopub.execute_input":"2024-09-17T14:16:47.067160Z","iopub.status.idle":"2024-09-17T14:16:47.385486Z","shell.execute_reply.started":"2024-09-17T14:16:47.067116Z","shell.execute_reply":"2024-09-17T14:16:47.384641Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Ignore warnings\n#logging.set_verbosity(logging.CRITICAL)\nfrom transformers import pipeline\n\n# Run text generation pipeline with our next model\nprompt = \"I fight with my parents daily..what can I do??\"\npipe = pipeline(task=\"text-generation\", model='/kaggle/working/Llama-finetune-therapy', tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:17:39.821677Z","iopub.execute_input":"2024-09-17T14:17:39.822461Z","iopub.status.idle":"2024-09-17T14:22:12.972706Z","shell.execute_reply.started":"2024-09-17T14:17:39.822418Z","shell.execute_reply":"2024-09-17T14:22:12.971662Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85a5b261282a4556bb82e5c8e0b025ad"}},"metadata":{}},{"name":"stdout","text":"<s>[INST] I fight with my parents daily..what can I do?? [/INST]  It's common for teenagers to have conflicts with their parents from time to time, but daily arguments can be challenging and emotionally draining. Here are some steps you can take to manage the situation:\n\n1. Communicate your feelings: Have an open and honest conversation with your parents about how you're feeling. Let them know what's bothering you and why you're feeling upset. Use \"I\" statements to express your feelings without blaming or attacking them.\n2. Set boundaries: It's important to set clear boundaries and expectations for how you want to be treated. Let your parents know what you're comfortable with and what you're not.\n3. Practice active listening: When you're in a conflict with your parents, make sure to listen actively to what they're saying.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:28:36.091454Z","iopub.execute_input":"2024-09-17T14:28:36.091875Z","iopub.status.idle":"2024-09-17T14:30:02.901049Z","shell.execute_reply.started":"2024-09-17T14:28:36.091837Z","shell.execute_reply":"2024-09-17T14:30:02.900172Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baf4f9f0a3764aa8a658fc377580e223"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  adapters_weights = torch.load(\n","output_type":"stream"}]},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:30:37.742006Z","iopub.execute_input":"2024-09-17T14:30:37.743083Z","iopub.status.idle":"2024-09-17T14:30:37.748210Z","shell.execute_reply.started":"2024-09-17T14:30:37.743025Z","shell.execute_reply":"2024-09-17T14:30:37.747158Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token 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\n\nmodel.push_to_hub(\"thrishala/mental_health_chatbot\", use_auth_token=\"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\", check_pr=True)\n\ntokenizer.push_to_hub(\"thrishala/mental_health_chatbot\", use_auth_token=\"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\", check_pr=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T14:39:41.190953Z","iopub.execute_input":"2024-09-17T14:39:41.191631Z","iopub.status.idle":"2024-09-17T14:43:55.773384Z","shell.execute_reply.started":"2024-09-17T14:39:41.191588Z","shell.execute_reply":"2024-09-17T14:43:55.772232Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:834: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d7d62a320cf47dbbcc206c50c1ec2dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8c082db9b9a4c368d187d6e503e1c98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38092c6f5cfb4314974e7781b6ad43bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cacec5b4ee24d12a3c32c73b4b687d3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:834: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b546ce2b102e4dbd93a858d27dce2a14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"688efd7b8c6e460e9c5014e5a99a930e"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/thrishala/mental_health_chatbot/commit/ec3434ede01390b5f580b939085e6c8cc5caddfb', commit_message='Upload tokenizer', commit_description='', oid='ec3434ede01390b5f580b939085e6c8cc5caddfb', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}